---
title: "Tidy text for EDH Inscriptions"
author: "Petra Hermankova"
date: "07/05/2020"
output: html_document
---


*Initial setup*
```{r setup, echo=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::install_github("mplex/cedhar", subdir="pkg/sdam")

library(tidyverse)
library(tidytext)
library(dplyr)
library(stringr)
library(sdam)
library(rjson)
library(getPass)
```

# Connect to Sciencedata.dk to access the dataset
1. Input your sciencedata.dk username - type directly into the RStudio console
```{r}
user <- readline("your sciencedata username: ")
```

2. Make the request (you will be asked for password in a new pop-up window)
```{r}
resp = request("EDH_clean_text_sample.json", path="/sharingin/648597@au.dk/SDAM_root/SDAM_data/EDH/", method="GET", cred=c(user, getPass("your sciencedata password: ")))
```

3. Make a list from the request and display first six records
```{r, tidy=TRUE}
list_json <- fromJSON(resp)
EDH_tibble = as_tibble(list_json)
head(EDH_tibble)
```

## OR access the local file
```{r}
#EDH_json_clean <- fromJSON(file = "../../outputs/EDH_clean_11_05_2020.json")
#EDH_tibble_clean <- as_tibble(EDH_json_clean)
```

```{r}
unique(EDH_tibble_clean$province_label[1:3])

```


# Tidy text analysis of the `clean_text_interpretive_word` column

## Tokenizing
```{r}
clean_interpretive_tokenized <- EDH_tibble_clean %>% 
  unnest_tokens(word, clean_text_interpretive, token = stringr::str_split, pattern = " ") %>% 
  drop_na(word) %>%
  print()
```

## Counting the most common words

```{r, tidy=TRUE}
clean_interpretive_tokenized %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 5000) %>% 
  mutate(word = reorder(word, n)) %>% 
  print()
```

## Grouping distribution of most freqent words by province

```{r}
clean_interpretive_tokenized %>% 
  count(province_label, word, sort = TRUE) %>% 
  group_by(province_label) %>% 
  summarise(total = sum(n)) -> clean_interpretive_total_province
clean_interpretive_total_province
```

