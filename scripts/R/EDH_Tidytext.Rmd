---
title: "Tidy text for EDH Inscriptions"
author: "Petra Hermankova"
date: "07/05/2020"
output: rmarkdown::github_document
---


*Initial setup*
```{r setup, echo=TRUE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::install_github("mplex/cedhar", subdir="pkg/sdam")

library(tidyverse)
library(tidytext)
library(dplyr)
library(stringr)
library(sdam)
library(rjson)
library(getPass)
```

# Connect to Sciencedata.dk to access the dataset
1. Input your sciencedata.dk username - type directly into the RStudio console
```{r}
user <- readline("your sciencedata username: ")
```

2. Make the request (you will be asked for password in a new pop-up window)
```{r}
resp = request("EDH_cleaned.json", path="/sharingin/648597@au.dk/SDAM_root/SDAM_data/EDH/", method="GET", cred=c(user, getPass("your sciencedata password: ")))
```

3. Make a list from the request
```{r, tidy=TRUE}
EDH_clean <- as_tibble(fromJSON(resp))
```

# Tidy text analysis of the `clean_text_interpretive_word` column

## Tokenizing
```{r}
clean_interpretive_tokenized <- EDH_clean %>% 
  unnest_tokens(word, clean_text_interpretive_word, token = stringr::str_split, pattern = " ") %>% 
  drop_na(word) %>%
  print()
```

## Counting the most common words

```{r, tidy=TRUE}
clean_interpretive_tokenized %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 5000) %>% 
  mutate(word = reorder(word, n)) %>% 
  print()
```

## Grouping distribution of most freqent words by province

```{r}
clean_interpretive_tokenized %>% 
  count(province_label_clean, word, sort = TRUE) %>% 
  group_by(province_label_clean) %>% 
  summarise(total = sum(n)) -> clean_interpretive_total_province
```

```{r, fig.height=10, fig.width=18}
clean_interpretive_total_province %>% 
  group_by() %>%
  ggplot(aes(total, province_label_clean)) +
  geom_col(fill = "darkblue") +
  theme_classic() +
  labs(x = "Number of inscriptions", y = "Province name", title = "Number of inscriptions per Roman province", subtitle = "EDH database, n = 80270")
```

